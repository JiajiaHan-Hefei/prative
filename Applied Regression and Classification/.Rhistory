gSRDF$X1 <- X1[,1]
gSRDF$X2 <- X1[,2]
gSRDF$X3 <- X1[,3]
bw <- gwr.sel(PctBach ~ X1 + X2 + X3, data=gSRDF, verbose=FALSE)
out <- gwr(PctBach ~ X1 + X2 + X3, data=gSRDF, bandwidth=bw, hatmatrix=TRUE)
out
spplot(gSRDF, "PctBach", col.regions=grey.colors(20))
spplot(gSRDF, c("X1", "X2", "X3"), col.regions=grey.colors(20))
# pattern in the local coefficients
spplot(out$SDF, c("X1", "X2", "X3"), col.regions=grey.colors(20))
# but no "significant" pattern
data(georgia)
g.adapt.gauss <- gwr.sel(PctBach ~ TotPop90 + PctRural + PctEld + PctFB +
PctPov + PctBlack, data=gSRDF, adapt=TRUE)
res.adpt <- gwr(PctBach ~ TotPop90 + PctRural + PctEld + PctFB + PctPov +
PctBlack, data=gSRDF, adapt=g.adapt.gauss)
res.adpt
pairs(as(res.adpt$SDF, "data.frame")[,2:8], pch=".")
brks <- c(-0.25, 0, 0.01, 0.025, 0.075)
cols <- grey(5:2/6)
plot(res.adpt$SDF, col=cols[findInterval(res.adpt$SDF$PctBlack, brks,
all.inside=TRUE)])
# simulation scenario with patterned dependent variable
set.seed(1)
X0 <- runif(nrow(gSRDF)*3)
X1 <- matrix(sample(X0), ncol=3)
X1 <- prcomp(X1, center=FALSE, scale.=FALSE)$x
gSRDF$X1 <- X1[,1]
gSRDF$X2 <- X1[,2]
gSRDF$X3 <- X1[,3]
bw <- gwr.sel(PctBach ~ X1 + X2 + X3, data=gSRDF, verbose=FALSE)
out <- gwr(PctBach ~ X1 + X2 + X3, data=gSRDF, bandwidth=bw, hatmatrix=TRUE)
out
spplot(gSRDF, "PctBach", col.regions=grey.colors(20))
spplot(gSRDF, c("X1", "X2", "X3"), col.regions=grey.colors(20))
# pattern in the local coefficients
spplot(out$SDF, c("X1", "X2", "X3"), col.regions=grey.colors(20))
# but no "significant" pattern
spplot(out$SDF, c("X1_se", "X2_se", "X3_se"), col.regions=grey.colors(20))
out$SDF$X1_t <- out$SDF$X1/out$SDF$X1_se
out$SDF$X2_t <- out$SDF$X2/out$SDF$X2_se
out$SDF$X3_t <- out$SDF$X3/out$SDF$X3_se
spplot(out$SDF, c("X1_t", "X2_t", "X3_t"), col.regions=grey.colors(20))
# simulation scenario with random dependent variable
yrn <- rnorm(nrow(gSRDF))
gSRDF$yrn <- sample(yrn)
bw <- gwr.sel(yrn ~ X1 + X2 + X3, data=gSRDF, verbose=FALSE)
# bandwidth selection maxes out at 620 km, equal to upper bound
# of line search
out <- gwr(yrn ~ X1 + X2 + X3, data=gSRDF, bandwidth=bw, hatmatrix=TRUE)
out
spplot(gSRDF, "yrn", col.regions=grey.colors(20))
spplot(gSRDF, c("X1", "X2", "X3"), col.regions=grey.colors(20))
# pattern in the local coefficients
spplot(out$SDF, c("X1", "X2", "X3"), col.regions=grey.colors(20))
# but no "significant" pattern
spplot(out$SDF, c("X1_se", "X2_se", "X3_se"), col.regions=grey.colors(20))
out$SDF$X1_t <- out$SDF$X1/out$SDF$X1_se
out$SDF$X3_t <- out$SDF$X3/out$SDF$X3_se
out$SDF$X2_t <- out$SDF$X2/out$SDF$X2_se
spplot(out$SDF, c("X1_t", "X2_t", "X3_t"), col.regions=grey.colors(20))
data(meuse)
coordinates(meuse) <- c("x", "y")
meuse$ffreq <- factor(meuse$ffreq)
georgia
data(georgia)
data(georgia)
georgia
data(georgia)
georgia
# simulation scenario with patterned dependent variable
set.seed(1)
X0 <- runif(nrow(gSRDF)*3)
X1 <- matrix(sample(X0), ncol=3)
X1 <- prcomp(X1, center=FALSE, scale.=FALSE)$x
gSRDF
data(georgia)
g.adapt.gauss <- gwr.sel(PctBach ~ TotPop90 + PctRural + PctEld + PctFB +
PctPov + PctBlack, data=gSRDF, adapt=TRUE)
res.adpt <- gwr(PctBach ~ TotPop90 + PctRural + PctEld + PctFB + PctPov +
PctBlack, data=gSRDF, adapt=g.adapt.gauss)
res.adpt
pairs(as(res.adpt$SDF, "data.frame")[,2:8], pch=".")
brks <- c(-0.25, 0, 0.01, 0.025, 0.075)
cols <- grey(5:2/6)
plot(res.adpt$SDF, col=cols[findInterval(res.adpt$SDF$PctBlack, brks,
all.inside=TRUE)])
data(columbus, package="spData")
col.lm <- lm(CRIME ~ INC + HOVAL, data=columbus)
summary(col.lm)
col.bw <- gwr.sel(CRIME ~ INC + HOVAL, data=columbus,
coords=cbind(columbus$X, columbus$Y))
col.gauss <- gwr(CRIME ~ INC + HOVAL, data=columbus,
coords=cbind(columbus$X, columbus$Y), bandwidth=col.bw, hatmatrix=TRUE)
col.gauss
col.gauss$SDF
dim(x0)
dim(X0)
# simulation scenario with patterned dependent variable
set.seed(1)
X0 <- runif(nrow(gSRDF)*3)
dim(X0)
size(X0)
length(X0)
X0 <- runif(nrow(500)*3)
X1 <- matrix(sample(X0), ncol=3)
dim(X1)
dim(X0)
lenth(X0)
length(X0)
X0 <- runif(nrow(500)*3)
length(X0)
?runif
?rep
rep(1:4, 2)
seq(0,1)
seq(100)
set.seed(1)
lon <- runif(500)
lat <- runif(500)
X0 <- runif(500)
X1 <- 3*runif(500)
beta <- rep(500)
for(i in 1:5)
{
beta(100*(i-1):100*i)=0.5*i
}
y <- beta%*%X1+X0
data <- data.frame(X1,X0,y)
bw <- gwr.sel(y ~ X1 + X0, data=data, coords=cbind(lon,lat))
out <- gwr(PctBach ~ X1 + X2 + X3, data=data,coords=cbind(lon,lat),bandwidth=bw, hatmatrix=TRUE)
# simulation scenario with patterned dependent variable
set.seed(1)
lon <- runif(500)
lat <- runif(500)
X0 <- runif(500)
X1 <- 3*runif(500)
beta <- rep(500)
for(i in 1:5)
{
beta(100*(i-1)+1:100*i)=0.5*i
}
y <- beta%*%X1+X0
beta
beta <- rep(1:500)
beta
set.seed(1)
lon <- runif(500)
lat <- runif(500)
X0 <- runif(500)
X1 <- 3*runif(500)
beta <- rep(1:500)
for(i in 1:5)
{
beta(100*(i-1)+1:100*i)=0.5*i
}
y <- beta%*%X1+X0
data <- data.frame(X1,X0,y)
y <- beta %*% X1+X0
set.seed(1)
lon <- runif(500)
lat <- runif(500)
X0 <- runif(500)
X1 <- 3*runif(500)
beta <- rep(1:500)
for(i in 1:5)
{
beta(100*(i-1)+1:100*i)=0.5*i
}
beta[1:3]
set.seed(1)
lon <- runif(500)
lat <- runif(500)
X0 <- runif(500)
X1 <- 3*runif(500)
beta <- rep(1:500)
for(i in 1:5)
{
beta[100*(i-1)+1:100*i]=0.5*i
}
y <- beta %*% X1+X0
size(beta)
length(beta)
set.seed(1)
lon <- runif(500)
lat <- runif(500)
X0 <- runif(500)
X1 <- 3*runif(500)
beta <- rep(1:500)
for(i in 1:5)
{
beta[100*(i-1)+1:100*i]=0.5*i
}
length(beta)
beta <- rep(1:500)
length(beta)
beta <- rep(1:500)
for(i in 1:5)
{
beta[100*(i-1)+1:100*i]=0.5*i
}
length(beta)
beta <- rep(1:500)
beta
beta[1:100]
beta[1:100]=0.5
beta[1:100]
beta[101:200]=1
beta[101:200]
beta
beta <- rep(1:500)
for(i in 1:5)
{
beta[100*(i-1)+1:100*i]=0.5*i
}
beta
i
beta <- rep(1:500)
for(i in 1:5)
{
beta[(100*(i-1)+1):(100*i)]=0.5*i
}
beta
# simulation scenario with patterned dependent variable
set.seed(1)
lon <- runif(500)
lat <- runif(500)
X0 <- runif(500)
X1 <- 3*runif(500)
beta <- rep(1:500)
for(i in 1:5)
{
beta[(100*(i-1)+1):(100*i)]=0.5*i
}
y <- beta %*% X1+X0
y
# simulation scenario with patterned dependent variable
set.seed(1)
lon <- rnorm(500)
lat <- rnorm(500)
X0 <- rnorm(500)
X1 <- 3*rnorm(500)
beta <- rep(1:500)
for(i in 1:5)
{
beta[(100*(i-1)+1):(100*i)]=0.5*i
}
y <- beta %*% X1+ X0
y
data <- data.frame(X1,X0,y)
bw <- gwr.sel(y ~ X1 + X0, data=data, coords=cbind(lon,lat))
out <- gwr(y ~ X1 + X2 + X3, data=data,coords=cbind(lon,lat),bandwidth=bw, hatmatrix=TRUE)
out <- gwr(y ~ X1 + X0, data=data,coords=cbind(lon,lat),bandwidth=bw, hatmatrix=TRUE)
spplot(data, "y", col.regions=grey.colors(20))
spplot(data, "y", col.regions=grey.colors(20))
spplot(data, c("X1", "X0"), col.regions=grey.colors(20))
gSRDF
# pattern in the local coefficients
spplot(out$SDF, c("X1", "X0"), col.regions=grey.colors(20))
spplot(data, c("X1", "X0"), col.regions=grey.colors(20))
data(columbus, package="spData")
col.lm <- lm(CRIME ~ INC + HOVAL, data=columbus)
summary(col.lm)
columbus
library(lars)
install.packages("lars")
install.packages("lars")
################共线性问题
library(lars)
data("diabetes")
w=data("diabetes")
kappa(w[,-1])
w=data("diabetes")[,11:75]
w
data("diabetes")
w=data("diabetes")
w
diabetes
w=diabetes[,11:75]
dim(diabetes)
w=as.matrix(diabetes)[,11:75]
w
kappa(w[,-1])
sort(vif(lm(y~.,w)),de=T)[1:5]
library(car)
install.packages("carData")
w=as.matrix(diabetes)[,11:75]
library(car)
kappa(w[,-1])#X2的条件数
sort(vif(lm(y~.,w)),de=T)[1:5]
library(car)
library(carData)
kappa(w[,-1])#X2的条件数
sort(vif(lm(y~.,w)),de=T)[1:5]
diabetes[,1]
diabetes[,11]
diabetes[,1]
diabetes[,2]
diabetes[1,]
sort(vif(lm(y~.,as.data.frame(w))),de=T)[1:5]
#############逐步回归
a=step(lm(y~.,as.data.frame(w)))
summary(a)
plot(a$fit,a$res)
abline(h=0,lty=2)
###########岭回归
library(ridge)
install.packages("ridge")
###########岭回归
library(ridge)
a=linearRidge(y~.,data=as.data.frame(w))
summary(a)
plot(a)
#########lasso回归
library(lars)
w=as.data.frame(w)
y=as.matrix(w[,1])
x2=as.matrix(w[,-1])
laa=lars(x2,y)#lars只用于矩阵型数据
plot(laa)
summary(laa)
cva=cv.lars(x2,y,K=10)#使用10折交叉验证
best=cva$index[which.min(cva$cv)]#选合适的比率
coef=coef.lars(laa,mode="fraction",s=best)
min(laa$Cp)#最小Cp
coef1=coef.lars(laa,mode="step",s=15)
########alasso回归
library(msgps)
y=w[,1]
x2=as.matrix(w[,-1])
al=msgps(x2,y,penalty = "alasso",gamma = 1,lambda = 0)
summary(al)
plot(al)
#######偏最小二乘回归
library(pls)
install.packages("pls")
#######偏最小二乘回归
library(pls)
ap=plsr(y~x2,64,validation="CV")
ap$loadings
ap$coef
RMSEP(ap)
MSEP(ap)
R2(ap)
par(mfrow=c(1,3))
plot(RMSEP(ap));abline(v=5,lty=2)
plot(MSEP(ap));abline(v=5,lty=2)
plot(R2(ap));abline(v=5,lty=2)
ap$loadings
##################CV函数
##随机把数据的下标分成Z份以做交叉验证使用
CV <- function(n,Z=10,seed=888){
z=rep(1:Z,ceiling(n/Z))[1:n]
set.seed(seed);z=sample(z,n)
mm=list()
for(i in 1:Z) mm[[i]]=(1:n)[z==i]
return(mm)
}
##########10折交叉验证
D=1
y=as.matrix(w[,D])
x2=as.matrix(w[,-D])
Z=10
mm=CV(n,Z)
MSEC=matrix(999,Z,6)
J=1
for(i in 1:Z){
m=mm[[i]]
M=mean((w[m,D]-mean(w[m,D]))^2)
a=lm(y~.,data=w[-m,])
MSEC[i,J]=mean((w[m,D]-perdict(a,w[m,D]))^2)/M
}
for(i in 1:Z){
m=mm[[i]]
M=mean((w[m,D]-mean(w[m,D]))^2)
a=lm(y~.,data=w[-m,])
MSEC[i,J]=mean((w[m,D]-predict(a,w[m,D]))^2)/M
}
##########10折交叉验证
n=nrow(w)
for(i in 1:Z){
m=mm[[i]]
M=mean((w[m,D]-mean(w[m,D]))^2)
a=lm(y~.,data=w[-m,])
MSEC[i,J]=mean((w[m,D]-predict(a,w[m,D]))^2)/M
}
i=1
m=mm[[i]]
M=mean((w[m,D]-mean(w[m,D]))^2)
a=lm(y~.,data=w[-m,])
MSEC[i,J]=mean((w[m,D]-predict(a,w[m,D]))^2)/M
##########10折交叉验证
w=as.matrix(w)
MSEC[i,J]=mean((w[m,D]-predict(a,w[m,D]))^2)/M
predict(a,w[m,D])
a
predict(a,w[m,D])
predict(a,w[m,])
##########10折交叉验证
w=as.data.frame(w)
predict(a,w[m,])
for(i in 1:Z){
m=mm[[i]]
M=mean((w[m,D]-mean(w[m,D]))^2)
a=lm(y~.,data=w[-m,])
MSEC[i,J]=mean((w[m,]-predict(a,w[m,]))^2)/M
}
J=1
for(i in 1:Z){
m=mm[[i]]
M=mean((w[m,D]-mean(w[m,D]))^2)
a=lm(y~.,data=w[-m,])
MSEC[i,J]=mean((y[m,]-predict(a,w[m,]))^2)/M
}
J=2
for(i in 1:Z){
m=mm[[i]]
M=mean((w[m,D]-mean(w[m,D]))^2)
a=step(y~.,data=w[-m,])
MSEC[i,J]=mean((y[m]-predict(a,w[m,D]))^2)/M
}
J=2
for(i in 1:Z){
m=mm[[i]]
M=mean((w[m,D]-mean(w[m,D]))^2)
a=step(y~.,data=w[-m,])
MSEC[i,J]=mean((y[m]-predict(a,w[m,]))^2)/M
}
J=2
for(i in 1:Z){
m=mm[[i]]
M=mean((w[m,D]-mean(w[m,D]))^2)
a=step(lm(y~.,data=w[-m,]))
MSEC[i,J]=mean((y[m]-predict(a,w[m,]))^2)/M
}
J=3
for(i in 1:Z){
m=mm[[i]]
M=mean((w[m,D]-mean(w[m,D]))^2)
a=linearRidge(y~.,data=w[-m,])
MSEC[i,J]=mean((y[m]-predict(a,w[m,]))^2)/M
}
J=4
sed.seed(1010)
for(i in 1:Z){
m=mm[[i]]
M=mean((w[m,D]-mean(w[m,D]))^2)
a=msgps(x2[-m,],y[-m],penalty = "lasso",gamma = 0,lambda = 0)
MSEC[i,J]=mean((y[m]-predict(a,x2[m,]))^2)/M
}
set.seed(1010)
for(i in 1:Z){
m=mm[[i]]
M=mean((w[m,D]-mean(w[m,D]))^2)
a=msgps(x2[-m,],y[-m],penalty = "lasso",gamma = 0,lambda = 0)
MSEC[i,J]=mean((y[m]-predict(a,x2[m,]))^2)/M
}
for(i in 1:Z){
m=mm[[i]]
M=mean((w[m,D]-mean(w[m,D]))^2)
a=msgps(x2[-m,],y[-m])
MSEC[i,J]=mean((y[m]-predict(a,x2[m,]))^2)/M
}
J=5
sed.seed(1010)
J=5
set.seed(1010)
for(i in 1:Z){
m=mm[[i]]
M=mean((w[m,D]-mean(w[m,D]))^2)
a=msgps(x2[-m,],y[-m],penalty = "alasso",gamma = 1,lambda = 0)
MSEC[i,J]=mean((y[m]-predict(a,x2[m,]))^2)/M
}
J=6
sed.seed(1010)
J=6
set.seed(1010)
for(i in 1:Z){
m=mm[[i]]
M=mean((w[m,D]-mean(w[m,D]))^2)
a=plsr(y[-m]~x2[-m,],5,validation = "CV")
MSEC[i,J]=mean((y[m]-predict(a,x2[m,]))^2)/M
}
MSEC=data.frame(MSEC)
names(MSEC)=c("lm","step","ridge","lasso","alasso","pls")
(NMSE=apply(MSEC,2,mean))
MSEC
(NMSE=apply(MSEC,2,mean))
w=read.table("seizure")
w=read.table("seizure")
w=read.table("seizure")
w=read.table("seizure.csv")
setwd('D://R语言程序编译/应用回归及分类练习')
w=read.table("seizure.csv")
setwd('D://R语言程序编译/应用回归及分类练习')
w=read.table("seizure.csv")
w=read.csv("seizure.csv")
setwd('D://R语言程序编译/应用回归及分类练习')
w=read.csv("seizure.csv")
?read.csv
setwd('D://R语言程序编译/应用回归及分类练习')
w=read.csv("seizure.csv")
w=read.csv("seizures.csv")
w
w=read.csv("seizures.csv")
library(lme4)
g=glmer(counts~time_1+time_2+time_3+time_4+treat+bcounts+age+(age|id),w,family = poisson)
g=glmer(counts~time_1+time_2+time_3+time_4+treat+counts+age+(age|id),w,family = poisson)
w=read.csv("seizures.csv")
library(lme4)
g=glmer(counts~time_1+time_2+time_3+time_4+treat+counts+age+(age|id),w,family = poisson)
g=glmer(counts~time_1+time_2+time_3+time_4+treat+counts+age+(age|id),w,family = poisson)
